{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import torch\n",
    "from validate.metric_utils import make_prediction_on_volume\n",
    "from validate.metric_utils import add_gaussian_noise\n",
    "import torch\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from os import walk\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "# define the models here\n",
    "from models.phirec.src import phirec_skip as phirec_skip\n",
    "from models.phirec.src.phirec_skip import PHISeg as PHiRec\n",
    "\n",
    "from models.punet.src import probabilistic_unet as probabilistic_unet\n",
    "from models.punet.src.probabilistic_unet import ProbabilisticUnet\n",
    "\n",
    "from models.unet.src import unet as unet\n",
    "from models.unet.src.unet import UNet as UNet\n",
    "\n",
    "from models.unet_dropout.src import unet as unet_dropout\n",
    "from models.unet_dropout.src.unet import UNet as UNet_dropout\n",
    "\n",
    "from models.unet_het.src import unet as unet_het\n",
    "from models.unet_het.src.unet import UNet as UNet_het\n",
    "\n",
    "from models.unet_het_dropout.src import unet as unet_het_dr\n",
    "from models.unet_het_dropout.src.unet import UNet as UNet_het_dr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference(us_factors, base_data_origin, split_file, model_names, settings, base_path_models, base_save_path):\n",
    "\n",
    "    # define model parameters\n",
    "    input_channels = 2 \n",
    "    num_classes = 2 \n",
    "    num_filters = [32, 64, 128, 192, 192, 192, 192]\n",
    "    for us_factor in us_factors:\n",
    "        # create input data list\n",
    "        files_origin = os.path.join(base_data_origin, us_factor)\n",
    "        print(files_origin)        \n",
    "\n",
    "        # alternative for only test data\n",
    "        with open(split_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "        f = [data['images'][i]['file_name'] for i in range(len(data['images']))]\n",
    "        print('files:', f)\n",
    "        print('len files:', len(f))\n",
    "\n",
    "        for model_name in model_names:\n",
    "            print('doing predictions for', model_name)\n",
    "            for setting in settings:\n",
    "                print('doing predictions for', setting)\n",
    "                model_path = os.path.join(base_path_models, model_name, setting)\n",
    "                model_file_path = []\n",
    "                for (dirpath, dirnames, filename_model) in walk(model_path):\n",
    "                    model_file_path.append(os.path.join(model_path, filename_model[0]))\n",
    "                    break\n",
    "                assert len(model_file_path) == 1\n",
    "                model_file_path = model_file_path[0]\n",
    "\n",
    "                # make distinction between the models where we are\n",
    "                if model_name == 'dropout':\n",
    "                    model = UNet_dropout(input_channels, num_classes)\n",
    "                    n_samples = 20\n",
    "                elif model_name == 'het':\n",
    "                    model = UNet_het(input_channels, num_classes)\n",
    "                    n_samples = 20\n",
    "                elif model_name == 'phirec':\n",
    "                    model = PHiRec(input_channels, num_classes, num_filters, image_size=(2,512,512))\n",
    "                    n_samples = 20\n",
    "                elif model_name == 'punet':\n",
    "                    model = ProbabilisticUnet(input_channels, num_classes, num_filters, image_size=(2,512,512))\n",
    "                    n_samples = 20\n",
    "                elif model_name == 'unet':\n",
    "                    model = UNet(input_channels, num_classes)\n",
    "                    n_samples = 1\n",
    "                elif model_name == 'het_dr':\n",
    "                    model = UNet_het_dr(input_channels, num_classes)\n",
    "                    n_samples = 20\n",
    "                else:\n",
    "                    print(model_name)\n",
    "                    raise ValueError('Not the right model loaded')\n",
    "                checkpoint = torch.load(model_file_path)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "                # loop over all data files and make predictions\n",
    "                for input_file in f:\n",
    "                    input_file_dir = os.path.join(files_origin, input_file)\n",
    "                    file = h5py.File(input_file_dir, 'r')\n",
    "                    vol_us = file['img_us'][()]\n",
    "                    print('vol us shape', vol_us.shape)\n",
    "                    file.close()\n",
    "                    reconstruction = make_prediction_on_volume(vol_us, model, n_samples)\n",
    "                    print('vol pred shape', reconstruction.shape)\n",
    "                    \n",
    "\n",
    "                    # create the save path\n",
    "                    save_path = os.path.join(base_save_path, us_factor[8:], model_name, setting, input_file)\n",
    "\n",
    "                    # save prediction in h5 file\n",
    "                    print(save_path)\n",
    "                    save_file = h5py.File(save_path, 'w')\n",
    "                    save_file.create_dataset('recon', data=reconstruction)\n",
    "                    save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path_models = '/mnt/qb/baumgartner/pfischer23/final_eval_skmtea/models'\n",
    "model_names = ['dropout', 'het', 'het_dr', 'phirec', 'punet', 'unet']\n",
    "base_save_path = '/mnt/qb/baumgartner/pfischer23/final_eval_skmtea/predictions/reconstructions'\n",
    "settings = ['train_4x', 'train_all']\n",
    "us_factors = ['skm-tea-4x', 'skm-tea-8x', 'skm-tea-16x']\n",
    "base_data_origin = '/mnt/qb/work/baumgartner/pfischer23/fastmriUQ'\n",
    "split_file = '/mnt/qb/baumgartner/rawdata/SKM-TEA/skm-tea/v1-release/annotations/v1.0.0/test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_inference(us_factors, base_data_origin, split_file, model_names, settings, base_path_models, base_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for the ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_ensembles(path_to_models, model, vol_us):\n",
    "    \"\"\"Generate samples from the ensembles\n",
    "\n",
    "    Args:\n",
    "        path_to_models (str): the path to where the models are saved\n",
    "        model (torch.module): the model to test for\n",
    "        vol_us (np.array): the undersampled volume with schape (x,y,z,2)\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # load all paths to the ensembles\n",
    "    model_paths = []\n",
    "    for path, subdirs, files in os.walk(path_to_models):\n",
    "        for name in files:\n",
    "            model_paths.append(os.path.join(path, name))\n",
    "    assert len(model_paths) == 20\n",
    "    \n",
    "    # loop over all models and perform predictions\n",
    "    final_shape = (20,) + vol_us.shape\n",
    "    samples = np.zeros(final_shape)\n",
    "    print(final_shape)\n",
    "    for i, model_path in enumerate(model_paths):\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        pred = make_prediction_on_volume(vol_us, model, n_samples=1)[0]\n",
    "        print(pred.shape)\n",
    "        samples[i] = pred\n",
    "    print(samples.shape)\n",
    "    return samples\n",
    "\n",
    "def perform_inference_ensembles(us_factors, base_data_origin, split_file, model_names, settings, base_path_models, base_save_path):\n",
    "    for us_factor in us_factors:\n",
    "        # create input data list\n",
    "        files_origin = os.path.join(base_data_origin, us_factor)\n",
    "        print(files_origin)\n",
    "        \n",
    "\n",
    "        # alternative for only test data\n",
    "        with open(split_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "        f = [data['images'][i]['file_name'] for i in range(len(data['images']))][-1:]\n",
    "        print('files:', f)\n",
    "        print('len files:', len(f))\n",
    "\n",
    "        for model_name in model_names:\n",
    "            print('doing predictions for', model_name)\n",
    "            for setting in settings:\n",
    "                print('doing predictions for', setting)\n",
    "                model_path = os.path.join(base_path_models, model_name, setting)\n",
    "\n",
    "                # define the model\n",
    "                model = UNet(2, 2)\n",
    "\n",
    "                # checkpoint = torch.load(model_file_path)\n",
    "                # model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "                # loop over all data files and make predictions\n",
    "                for input_file in f:\n",
    "                    input_file_dir = os.path.join(files_origin, input_file)\n",
    "                    file = h5py.File(input_file_dir, 'r')\n",
    "                    vol_us = file['img_us'][()]\n",
    "                    print('vol us shape', vol_us.shape)\n",
    "                    file.close()\n",
    "                    reconstruction = generate_samples_ensembles(model_path, model, vol_us) # make_prediction_on_volume(vol_us, model, n_samples)\n",
    "                    print('vol pred shape', reconstruction.shape)\n",
    "                    \n",
    "\n",
    "                    # create the save path\n",
    "                    save_path = os.path.join(base_save_path, us_factor[8:], model_name, setting, input_file)\n",
    "\n",
    "                    # save prediction in h5 file\n",
    "                    print(save_path)\n",
    "                    save_file = h5py.File(save_path, 'w')\n",
    "                    save_file.create_dataset('recon', data=reconstruction)\n",
    "                    save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_inference_ensembles(us_factors, base_data_origin, split_file, model_names, settings, base_path_models, base_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate SSIM/PSNR for the reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validate.metric_utils import eval_ssim_psnr_big\n",
    "model_names = ['dropout', 'het', 'het_dr', 'phirec', 'punet', 'unet', 'ensemble']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ssim_psnr_big(us_factors, base_data_origin, model_names, settings, base_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Reconstruction NCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validate.metric_utils import eval_ncc_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ncc_big(us_factors, base_data_origin, model_names, settings, base_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validate.metric_utils import eval_var_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_var_big(us_factors, model_names, settings, base_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform segmentation inference for the reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validate.metric_utils import make_prediction_on_volume_segmentation\n",
    "def perform_inference_segm(us_factors, base_data_origin, split_file, model_names, settings, model_path, base_save_path):\n",
    "    for us_factor in us_factors:\n",
    "        # create input data list\n",
    "        input_channels = 2 \n",
    "        num_classes = 7 \n",
    "        \n",
    "        model = UNet(input_channels, num_classes)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "        # alternative for only test data\n",
    "        with open(split_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "        f = [data['images'][i]['file_name'] for i in range(len(data['images']))]\n",
    "        print('files:', f)\n",
    "        print('len files:', len(f))\n",
    "\n",
    "        for model_name in model_names:\n",
    "            print('doing predictions for', model_name)\n",
    "            for setting in settings:\n",
    "                # loop over all data files and make predictions\n",
    "                for input_file in f:\n",
    "                    input_file_dir = os.path.join(base_data_origin, us_factor, model_name, setting, input_file)\n",
    "\n",
    "                    file = h5py.File(input_file_dir, 'r')\n",
    "                    data = file['recon'][()]\n",
    "                    print('vol data shape', data.shape)\n",
    "                    file.close()\n",
    "                    # reconstruction = make_prediction_on_volume(vol_us, model, n_samples)\n",
    "                    segmentation = make_prediction_on_volume_segmentation(data, model)\n",
    "                    print('vol segm shape', segmentation.shape)\n",
    "                    \n",
    "\n",
    "                    # create the save path\n",
    "                    save_path = os.path.join(base_save_path, us_factor, model_name, setting, input_file)\n",
    "\n",
    "                    # save prediction in h5 file\n",
    "                    print(save_path)\n",
    "                    save_file = h5py.File(save_path, 'w')\n",
    "                    save_file.create_dataset('segm', data=segmentation)\n",
    "                    save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'path/to/your/segmentation/model.pth'\n",
    "perform_inference_segm(us_factors, base_data_origin, split_file, model_names, settings, model_path, base_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate NCC for the segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validate.metric_utils import eval_ncc_big_segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ncc_big_segmentations(us_factors, base_data_origin, model_names, settings, base_save_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
